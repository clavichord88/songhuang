<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <title>Song Huang | AWS</title>
    <link rel="stylesheet" href="style.css" />
</head>
<body>

  <header>
    <nav>
      <a href="index.html">Home</a>
      <a href="projects.html">Projects</a>
      <a href="index.html#contact">Contact</a>
    </nav>
  </header>

  <main class="container">
    
    <div class="project">
      <h2>Introduction to LLM</h2>
      <h5>Difference between LLM and NLP:</h5>
      <ul>
        <li>LLMs are trained on vast quantities of text data. This large-scale training allows LLMs to capture deeper contextual information and subtleties of human language compared to previous approaches. As a result, LLMs have significantly improved performance in a wide range of NLP tasks, including text translation, sentiment analysis, question answering, and many more.</li>
        <li>Earlier NLP models were typically designed for specific tasks, such as text categorization, language translation, etc. While those earlier NLP models excelled in their narrow applications, LLMs demonstrate a broader proficiency across a wide range of NLP tasks.</li>
      </ul>
      <h5>What is an LLM?</h5>
      <p>An LLM is a neural network designed to understand, generate, and respond to human-like text. These models are deep neural networks trained on massive amounts of text data, sometimes encompassing large portions of the entire publicly available text on the internet. The “large” in “large language model” refers to both the model's size in terms of parameters and the immense dataset on which it's trained. LLMs utilize an architecture called the <strong>transformer</strong>, which allows them to pay selective attention to different parts of the input when making predictions, making them especially adept at handling the nuances and complexities of human language.</p>
      <!-- <a class="button" href="llm/s3.html" target="_blank"></a> -->
      <h5>Stages of building and using LLMs</h5>
      <p>The general process of creating an LLM includes pretraining and fine-tuning.</p>
      <ul>
        <li>The “pre” in “pretraining” refers to the initial phase where a model like an LLM is trained on a large, diverse dataset to develop a broad understanding of language.</li>
        <li>This pretrained model then serves as a foundational resource that can be further refined through fine-tuning, a process where the model is specifically trained on a narrower dataset that is more specific to particular tasks or domains. </li>
      </ul>
      <img src="static/imgs/llmprocess.jpg" alt="LLM_process">
      <p>The two most popular categories of fine-tuning LLMs are instruction fine-tuning and classification fine-tuning. </p>
      <ul>
        <li>In instruction fine-tuning, the labeled dataset consists of instruction and answer pairs, such as a query to translate a text accompanied by the correctly translated text.</li>
        <li>In classification fine-tuning, the labeled dataset consists of texts and associated class labels—for example, emails associated with “spam” and “not spam” labels.</li>
      </ul> 
      <h5>Transformer Architecture</h5>
      <p>The transformer architecture consists of two submodules: an encoder and a decoder. The encoder module processes the input text and encodes it into a series of numerical representations or vectors that capture the contextual information of the input. Then, the decoder module takes these encoded vectors and generates the output text. Both the encoder and decoder consist of many layers connected by a so-called self-attention mechanism, which allows the model to weigh the importance of different words or tokens in a sequence relative to each other. This mechanism enables the model to capture long-range dependencies and contextual relationships within the input data, enhancing its ability to generate coherent and contextually relevant output.</p>
      <p>BERT (short for bidirectional encoder representations from transformers) which is built upon the original transformer’s encoder submodule, differs in its training approach from GPT (short for generative pretrained transformers). While GPT is designed for generative tasks, BERT and its variants specialize in masked word prediction, where the model predicts masked or hidden words in a given sentence. GPT, on the other hand, focuses on the decoder portion of the original transformer architecture and is designed for tasks that require generating texts. This includes machine translation, text summarization, fiction writing, writing computer code, and more.</p>
      <img src="static/imgs/buildllm.jpg" alt="llm">
    </div>
  </main>

  <footer>
    &copy; 2025 Your Name. All rights reserved.
  </footer>

</body>
</html>
