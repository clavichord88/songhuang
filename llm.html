<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <title>Song Huang | AWS</title>
    <link rel="stylesheet" href="style.css" />
</head>
<body>

  <header>
    <nav>
      <a href="index.html">Home</a>
      <a href="projects.html">Projects</a>
      <a href="index.html#contact">Contact</a>
    </nav>
  </header>

  <main class="container">
    
    <div class="project">
      <h2>Introduction to LLM</h2>
      <h5>Difference between LLM and NLP:</h5>
      <ul>
        <li>LLMs are trained on vast quantities of text data. This large-scale training allows LLMs to capture deeper contextual information and subtleties of human language compared to previous approaches. As a result, LLMs have significantly improved performance in a wide range of NLP tasks, including text translation, sentiment analysis, question answering, and many more.</li>
        <li>Earlier NLP models were typically designed for specific tasks, such as text categorization, language translation, etc. While those earlier NLP models excelled in their narrow applications, LLMs demonstrate a broader proficiency across a wide range of NLP tasks.</li>
      </ul>
      <h5>What is an LLM?</h5>
      <p>An LLM is a neural network designed to understand, generate, and respond to human-like text. These models are deep neural networks trained on massive amounts of text data, sometimes encompassing large portions of the entire publicly available text on the internet. The “large” in “large language model” refers to both the model's size in terms of parameters and the immense dataset on which it's trained. LLMs utilize an architecture called the <strong>transformer</strong>, which allows them to pay selective attention to different parts of the input when making predictions, making them especially adept at handling the nuances and complexities of human language.</p>
      <!-- <a class="button" href="llm/s3.html" target="_blank"></a> -->
      <h5>Stages of building and using LLMs</h5>
      <p>The general process of creating an LLM includes pretraining and fine-tuning.</p>
      <ul>
        <li>The “pre” in “pretraining” refers to the initial phase where a model like an LLM is trained on a large, diverse dataset to develop a broad understanding of language.</li>
        <li>This pretrained model then serves as a foundational resource that can be further refined through fine-tuning, a process where the model is specifically trained on a narrower dataset that is more specific to particular tasks or domains. </li>
      </ul>
      <img src="static/imgs/llmprocess.jpg" alt="LLM_process">
    </div>

    
  </main>

  <footer>
    &copy; 2025 Your Name. All rights reserved.
  </footer>

</body>
</html>
